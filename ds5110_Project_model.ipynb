{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col, when, count, lit, regexp_replace\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "infile = 'H1B_combined_final.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"immi\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(infile, inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------------+-------------+----------+--------------------+----------+--------------------+------------------+--------------------+-------------+--------------+---------------------+----------------+---------------+--------------+\n",
      "|       CASE_NUMBER|        CASE_STATUS|RECEIVED_DATE|DECISION_DATE|VISA_CLASS|           JOB_TITLE|  SOC_CODE|           SOC_TITLE|FULL_TIME_POSITION|       EMPLOYER_NAME|EMPLOYER_CITY|EMPLOYER_STATE|WAGE_RATE_OF_PAY_FROM|WAGE_UNIT_OF_PAY|PREVAILING_WAGE|PW_UNIT_OF_PAY|\n",
      "+------------------+-------------------+-------------+-------------+----------+--------------------+----------+--------------------+------------------+--------------------+-------------+--------------+---------------------+----------------+---------------+--------------+\n",
      "|I-200-12240-490687|CERTIFIED-WITHDRAWN|   2012-08-27|   2015-10-16|      H-1B|POSTDOCTORAL RESE...|10-1021.00|BIOCHEMISTS AND B...|              null|UNIVERSITY OF MIC...|    ANN ARBOR|            MI|              41000.0|            Year|        36067.0|          Year|\n",
      "|I-200-13053-847481|CERTIFIED-WITHDRAWN|   2013-02-25|   2016-01-13|      H-1B|CHIEF OPERATING O...|   11-1011|    CHIEF EXECUTIVES|              null|GOODMAN NETWORKS,...|        PLANO|            TX|             400000.0|            Year|       242674.0|          Year|\n",
      "|I-200-13088-054259|CERTIFIED-WITHDRAWN|   2013-04-23|   2015-12-10|      H-1B|CHIEF PROCESS OFF...|   11-1011|    CHIEF EXECUTIVES|              null|PORTS AMERICA GRO...|  JERSEY CITY|            NJ|             264000.0|            Year|       193066.0|          Year|\n",
      "|I-200-13144-034110|CERTIFIED-WITHDRAWN|   2013-05-24|   2016-01-19|      H-1B|REGIONAL PRESIDEN...|   11-1011|    CHIEF EXECUTIVES|              null|GATES CORPORATION...|       DENVER|            CO|             220314.0|            Year|       220314.0|          Year|\n",
      "|I-200-13172-415116|          WITHDRAWN|   2013-06-26|   2016-05-20|      H-1B|PRESIDENT MONGOLI...|   11-1011|    CHIEF EXECUTIVES|              null|PEABODY INVESTMEN...|    ST. LOUIS|            MO|               171.63|            Hour|          75.73|          Hour|\n",
      "|I-200-13219-306739|CERTIFIED-WITHDRAWN|   2013-08-07|   2015-10-30|      H-1B|EXECUTIVE V P, GL...|   11-1011|    CHIEF EXECUTIVES|              null|BURGER KING CORPO...|        MIAMI|            FL|             225000.0|            Year|       225000.0|          Year|\n",
      "|I-200-13317-861241|CERTIFIED-WITHDRAWN|   2013-11-20|   2016-02-22|      H-1B|CHIEF OPERATING O...|   11-1011|    CHIEF EXECUTIVES|              null|BT AND MK ENERGY ...|        MIAMI|            FL|             120000.0|            Year|        91021.0|          Year|\n",
      "|I-200-14037-755844|CERTIFIED-WITHDRAWN|   2014-02-07|   2015-11-16|      H-1B|CHIEF OPERATIONS ...|   11-1011|    CHIEF EXECUTIVES|              null|GLOBO MOBILE TECH...|     SAN JOSE|            CA|             150000.0|            Year|       150000.0|          Year|\n",
      "|I-200-14042-183163|CERTIFIED-WITHDRAWN|   2014-02-19|   2015-11-18|      H-1B|           PRESIDENT|   11-1011|    CHIEF EXECUTIVES|              null|  ESI COMPANIES INC.|      MEMPHIS|            TN|             175000.0|            Year|       127546.0|          Year|\n",
      "|I-200-14072-690535|          WITHDRAWN|   2014-03-19|   2016-03-03|      H-1B|           PRESIDENT|   11-1011|    CHIEF EXECUTIVES|              null|LESSARD INTERNATI...|       VIENNA|            VA|             155000.0|            Year|       154648.0|          Year|\n",
      "+------------------+-------------------+-------------+-------------+----------+--------------------+----------+--------------------+------------------+--------------------+-------------+--------------+---------------------+----------------+---------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-----+\n",
      "|EMPLOYER_NAME                                    |count|\n",
      "+-------------------------------------------------+-----+\n",
      "|FIVEPALLS, INC                                   |3    |\n",
      "|TERRATHERM, INC                                  |1    |\n",
      "|SHIPPAN RACQUET CLUB                             |1    |\n",
      "|VAN PARYS ARCHITECTURE AND DESIGN LLP            |1    |\n",
      "|LANDED, INC.                                     |1    |\n",
      "|ADEQUALITY, INC.                                 |4    |\n",
      "|PIRAMAL CRITICAL CARE, INC.                      |9    |\n",
      "|LOOKBOOKER, INC.                                 |1    |\n",
      "|C1X INC                                          |2    |\n",
      "|BAXTER HEALTHCARE CORPORATION                    |69   |\n",
      "|STELLA & DOT, LLC                                |1    |\n",
      "|NEWGEN SOFTWARE INC.                             |71   |\n",
      "|NEW IEM POWER SYSTEMS, LLC, DBA IEM POWER SYSTEMS|4    |\n",
      "|ANIMOTO INC.                                     |13   |\n",
      "|MY BLUEBONNET COMPANY, LLC                       |3    |\n",
      "|NBTY, INC.                                       |11   |\n",
      "|IRON FIST INTERNATIONAL, INC.                    |1    |\n",
      "|NEILMED PHARMACEUTICALS INC.                     |1    |\n",
      "|ORIGAL LLP                                       |4    |\n",
      "|THE APPOINTMENT GROUP LLC                        |7    |\n",
      "+-------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_grp = df.groupBy(\"EMPLOYER_NAME\").count()\n",
    "emp_grp.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233575"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_grp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|PW_UNIT_OF_PAY|  count|\n",
      "+--------------+-------+\n",
      "|          Year|2791861|\n",
      "|     Bi-Weekly|    362|\n",
      "|         Month|   1628|\n",
      "|          Hour| 187659|\n",
      "|          null|   2157|\n",
      "|          Week|    600|\n",
      "|       61360.0|      1|\n",
      "|       82867.0|      1|\n",
      "+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prevailing wage is in different units(yearly,monthly,hourly etc)\n",
    "df.groupBy(\"PW_UNIT_OF_PAY\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|WAGE_UNIT_OF_PAY|  count|\n",
      "+----------------+-------+\n",
      "|            Year|2792985|\n",
      "|       Bi-Weekly|    656|\n",
      "|           Month|   2521|\n",
      "|            Hour| 187078|\n",
      "|            null|     74|\n",
      "|        150000.0|      1|\n",
      "|         70000.0|      1|\n",
      "|         88275.0|      1|\n",
      "|            Week|    950|\n",
      "|        155106.0|      1|\n",
      "|              MA|      1|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Actual wage is in different units(yearly,monthly,hourly etc)\n",
    "df.groupBy(\"WAGE_UNIT_OF_PAY\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------+---------------------+--------------+\n",
      "|SOC_CODE|EMPLOYER_NAME|EMPLOYER_STATE|WAGE_RATE_OF_PAY_FROM|PW_UNIT_OF_PAY|\n",
      "+--------+-------------+--------------+---------------------+--------------+\n",
      "|      21|          138|           178|                   43|          2157|\n",
      "+--------+-------------+--------------+---------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Finding NULL columns in the selected features\n",
    "df_Columns=[\"SOC_CODE\",\"EMPLOYER_NAME\",\"EMPLOYER_STATE\",\"WAGE_RATE_OF_PAY_FROM\",\"PW_UNIT_OF_PAY\"]\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df_Columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copnverting unit of prevailing wages into a single unit,which is monthly\n",
    "df = df.withColumn(\"pw_pay\", when((df.PW_UNIT_OF_PAY == 'Bi-Weekly'), lit(df.PREVAILING_WAGE * 26))\n",
    "              .when((df.PW_UNIT_OF_PAY == 'Month'), lit(df.PREVAILING_WAGE * 12))\n",
    "             .when((df.PW_UNIT_OF_PAY == 'Hour'), lit(df.PREVAILING_WAGE * 40 * 52))\n",
    "             .when((df.PW_UNIT_OF_PAY == 'Week'), lit(df.PREVAILING_WAGE * 52))\n",
    "             .otherwise(lit(df.PREVAILING_WAGE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copnverting unit of actual wages into a single unit,which is monthly\n",
    "df = df.withColumn(\"pay\", when((df.WAGE_UNIT_OF_PAY == 'Bi-Weekly'), lit(df.WAGE_RATE_OF_PAY_FROM * 26))\n",
    "              .when((df.WAGE_UNIT_OF_PAY == 'Month'), lit(df.WAGE_RATE_OF_PAY_FROM * 12))\n",
    "             .when((df.WAGE_UNIT_OF_PAY == 'Hour'), lit(df.WAGE_RATE_OF_PAY_FROM * 40 * 52))\n",
    "             .when((df.WAGE_UNIT_OF_PAY == 'Week'), lit(df.WAGE_RATE_OF_PAY_FROM * 52))\n",
    "             .otherwise(lit(df.WAGE_RATE_OF_PAY_FROM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+------------------+------------------+-------------------+\n",
      "|  SOC_CODE|       EMPLOYER_NAME|EMPLOYER_STATE|               pay|            pw_pay|        CASE_STATUS|\n",
      "+----------+--------------------+--------------+------------------+------------------+-------------------+\n",
      "|10-1021.00|UNIVERSITY OF MIC...|            MI|           41000.0|           36067.0|CERTIFIED-WITHDRAWN|\n",
      "|   11-1011|GOODMAN NETWORKS,...|            TX|          400000.0|          242674.0|CERTIFIED-WITHDRAWN|\n",
      "|   11-1011|PORTS AMERICA GRO...|            NJ|          264000.0|          193066.0|CERTIFIED-WITHDRAWN|\n",
      "|   11-1011|GATES CORPORATION...|            CO|          220314.0|          220314.0|CERTIFIED-WITHDRAWN|\n",
      "|   11-1011|PEABODY INVESTMEN...|            MO|356990.39999999997|157518.40000000002|          WITHDRAWN|\n",
      "|   11-1011|BURGER KING CORPO...|            FL|          225000.0|          225000.0|CERTIFIED-WITHDRAWN|\n",
      "|   11-1011|BT AND MK ENERGY ...|            FL|          120000.0|           91021.0|CERTIFIED-WITHDRAWN|\n",
      "|   11-1011|GLOBO MOBILE TECH...|            CA|          150000.0|          150000.0|CERTIFIED-WITHDRAWN|\n",
      "|   11-1011|  ESI COMPANIES INC.|            TN|          175000.0|          127546.0|CERTIFIED-WITHDRAWN|\n",
      "|   11-1011|LESSARD INTERNATI...|            VA|          155000.0|          154648.0|          WITHDRAWN|\n",
      "+----------+--------------------+--------------+------------------+------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feats = [\"SOC_CODE\", \n",
    "         \"EMPLOYER_NAME\", \n",
    "         \"EMPLOYER_STATE\", \n",
    "         \"pay\",\n",
    "        \"pw_pay\"]\n",
    "\n",
    "# subset the dataframe on these predictors\n",
    "df1 = df.select(*feats,'CASE_STATUS')\n",
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------+------+---+-----------+\n",
      "|SOC_CODE|EMPLOYER_NAME|EMPLOYER_STATE|pw_pay|pay|CASE_STATUS|\n",
      "+--------+-------------+--------------+------+---+-----------+\n",
      "|      21|          138|           178|  2008| 43|          0|\n",
      "+--------+-------------+--------------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Finding NULL columns in the selected features\n",
    "df_Columns=[\"SOC_CODE\",\"EMPLOYER_NAME\",\"EMPLOYER_STATE\",\"pw_pay\",\"pay\",\"CASE_STATUS\"]\n",
    "df1.select([count(when(col(c).isNull(), c)).alias(c) for c in df_Columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropped the null columns\n",
    "df1=df1.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------+------+---+-----------+\n",
      "|SOC_CODE|EMPLOYER_NAME|EMPLOYER_STATE|pw_pay|pay|CASE_STATUS|\n",
      "+--------+-------------+--------------+------+---+-----------+\n",
      "|       0|            0|             0|     0|  0|          0|\n",
      "+--------+-------------+--------------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verifying if there are NULL columns\n",
    "df_Columns=[\"SOC_CODE\",\"EMPLOYER_NAME\",\"EMPLOYER_STATE\",\"pw_pay\",\"pay\",\"CASE_STATUS\"]\n",
    "df1.select([count(when(col(c).isNull(), c)).alias(c) for c in df_Columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|         CASE_STATUS|  count|\n",
      "+--------------------+-------+\n",
      "|              Denied|   6674|\n",
      "|           CERTIFIED|1694655|\n",
      "| CERTIFIED-WITHDRAWN| 105125|\n",
      "|Certified - Withd...|  31448|\n",
      "|           Withdrawn|  20608|\n",
      "|           Certified|1036456|\n",
      "|           WITHDRAWN|  60779|\n",
      "|              DENIED|  26283|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Case status needs cleaning. Some values are in caps and some are in smalls\n",
    "df1.groupBy(\"CASE_STATUS\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+------------------+------------------+-------------------+\n",
      "|  SOC_CODE|       EMPLOYER_NAME|EMPLOYER_STATE|               pay|            pw_pay|        CASE_STATUS|\n",
      "+----------+--------------------+--------------+------------------+------------------+-------------------+\n",
      "|10-1021.00|UNIVERSITY OF MIC...|            MI|           41000.0|           36067.0|certified-withdrawn|\n",
      "|   11-1011|GOODMAN NETWORKS,...|            TX|          400000.0|          242674.0|certified-withdrawn|\n",
      "|   11-1011|PORTS AMERICA GRO...|            NJ|          264000.0|          193066.0|certified-withdrawn|\n",
      "|   11-1011|GATES CORPORATION...|            CO|          220314.0|          220314.0|certified-withdrawn|\n",
      "|   11-1011|PEABODY INVESTMEN...|            MO|356990.39999999997|157518.40000000002|          withdrawn|\n",
      "+----------+--------------------+--------------+------------------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Changing all vaues to small letters\n",
    "df1 = df1.withColumn(\"CASE_STATUS\",f.lower(f.col(\"CASE_STATUS\")))\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|         CASE_STATUS|  count|\n",
      "+--------------------+-------+\n",
      "|              denied|  32957|\n",
      "|           withdrawn|  81387|\n",
      "| certified-withdrawn| 105125|\n",
      "|certified - withd...|  31448|\n",
      "|           certified|2731111|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Needs further cleaning \n",
    "df1.groupBy(\"CASE_STATUS\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing all certified-withdrawn cases to \"certified\"\n",
    "df2 = df1.withColumn('CASE_STATUS', when(df1.CASE_STATUS == 'certified-withdrawn',regexp_replace(df1.CASE_STATUS,'certified-withdrawn','certified'))\n",
    "                     .when(df1.CASE_STATUS == 'certified - withdrawn',regexp_replace(df1.CASE_STATUS,'certified - withdrawn','certified'))\n",
    "                     .otherwise(df1.CASE_STATUS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|CASE_STATUS|  count|\n",
      "+-----------+-------+\n",
      "|     denied|  32957|\n",
      "|  withdrawn|  81387|\n",
      "|  certified|2867684|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Target variable is clean now with 3 classes\n",
    "df2.groupBy(\"CASE_STATUS\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding label feature for the 3 classes\n",
    "df2 = df2.withColumn(\"label\", when((df2.CASE_STATUS == 'denied'), 0)\n",
    "                    .when((df2.CASE_STATUS == 'certified'), 1)\n",
    "                     .otherwise(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+------------------+------------------+-----------+-----+\n",
      "|  SOC_CODE|       EMPLOYER_NAME|EMPLOYER_STATE|               pay|            pw_pay|CASE_STATUS|label|\n",
      "+----------+--------------------+--------------+------------------+------------------+-----------+-----+\n",
      "|10-1021.00|UNIVERSITY OF MIC...|            MI|           41000.0|           36067.0|  certified|    1|\n",
      "|   11-1011|GOODMAN NETWORKS,...|            TX|          400000.0|          242674.0|  certified|    1|\n",
      "|   11-1011|PORTS AMERICA GRO...|            NJ|          264000.0|          193066.0|  certified|    1|\n",
      "|   11-1011|GATES CORPORATION...|            CO|          220314.0|          220314.0|  certified|    1|\n",
      "|   11-1011|PEABODY INVESTMEN...|            MO|356990.39999999997|157518.40000000002|  withdrawn|    2|\n",
      "+----------+--------------------+--------------+------------------+------------------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|2867684|\n",
      "|    2|  81387|\n",
      "|    0|  32957|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SOC_CODE', 'string'),\n",
       " ('EMPLOYER_NAME', 'string'),\n",
       " ('EMPLOYER_STATE', 'string'),\n",
       " ('pay', 'string'),\n",
       " ('pw_pay', 'string'),\n",
       " ('CASE_STATUS', 'string'),\n",
       " ('label', 'int')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the feature \"pay\" from string to doubletype\n",
    "df2=df2.withColumn(\"pay\",df.pay.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the feature \"pw_pay\" from string to doubletype\n",
    "df2=df2.withColumn(\"pw_pay\",df.pw_pay.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SOC_CODE', 'string'),\n",
       " ('EMPLOYER_NAME', 'string'),\n",
       " ('EMPLOYER_STATE', 'string'),\n",
       " ('pay', 'double'),\n",
       " ('pw_pay', 'double'),\n",
       " ('CASE_STATUS', 'string'),\n",
       " ('label', 'int')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------------------+------------------+-----+\n",
      "|EMPLOYER_STATE|  SOC_CODE|            pw_pay|               pay|label|\n",
      "+--------------+----------+------------------+------------------+-----+\n",
      "|            MI|10-1021.00|           36067.0|           41000.0|    1|\n",
      "|            TX|   11-1011|          242674.0|          400000.0|    1|\n",
      "|            NJ|   11-1011|          193066.0|          264000.0|    1|\n",
      "|            CO|   11-1011|          220314.0|          220314.0|    1|\n",
      "|            MO|   11-1011|157518.40000000002|356990.39999999997|    2|\n",
      "|            FL|   11-1011|          225000.0|          225000.0|    1|\n",
      "|            FL|   11-1011|           91021.0|          120000.0|    1|\n",
      "|            CA|   11-1011|          150000.0|          150000.0|    1|\n",
      "|            TN|   11-1011|          127546.0|          175000.0|    1|\n",
      "|            VA|   11-1011|          154648.0|          155000.0|    2|\n",
      "+--------------+----------+------------------+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Variable with the features selected to build the model\n",
    "ftrs = [\"EMPLOYER_STATE\",\n",
    "        \"SOC_CODE\",\n",
    "        \"pw_pay\",\n",
    "         \"pay\"]\n",
    "\n",
    "# subset the dataframe on these predictors\n",
    "df3 = df2.select(*ftrs,'label')\n",
    "df3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into 80% training and 20% test data\n",
    "seed = 314\n",
    "train_test = [0.8, 0.2]\n",
    "training, test = df3.randomSplit(train_test, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable Function with preprocessing, model build and evaluation steps \n",
    "def model(ev):\n",
    "   #Feature Preprocessing steps\n",
    "   categoricalCols = [field for (field, dataType) in training.dtypes if dataType == \"string\"]\n",
    "   indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "   oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "   stringIndexer = StringIndexer(inputCols=categoricalCols,outputCols=indexOutputCols,handleInvalid=\"skip\")\n",
    "   oheEncoder = OneHotEncoder(inputCols=indexOutputCols,outputCols=oheOutputCols)\n",
    "   numericCols = [field for (field, dataType) in training.dtypes if ((dataType == \"double\") & (field != \"label\"))]\n",
    "   assemblerInputs = oheOutputCols + numericCols\n",
    "   va = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\", handleInvalid=\"skip\") \n",
    "\n",
    "   # Model Build using pipelines\n",
    "   pipeline = Pipeline(stages=[stringIndexer,oheEncoder,va, ev])\n",
    "   model = pipeline.fit(training)\n",
    "    \n",
    "   #Model Evaluation using MulticlassClassificationEvaluator and printing stats for the model\n",
    "   preds = model.transform(test)\n",
    "   mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "   acc = mcEvaluator.evaluate(preds)\n",
    "   predictionAndLabels = model.transform(test).select('label', 'prediction')\n",
    "   metrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))\n",
    "   confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "   labels = [int(l) for l in metrics.call('labels')]\n",
    "   f1 = metrics.fMeasure(1.0)\n",
    "   precision = metrics.precision(1.0)\n",
    "   recall = metrics.recall(1.0)\n",
    "   data = metrics.confusionMatrix().toArray()\n",
    "   rdd1 = spark.sparkContext.parallelize(data)\n",
    "   rdd2 = rdd1.map(lambda x: [int(i) for i in x])\n",
    "   df = rdd2.toDF()\n",
    "   print(\"Printing Confusion Matrix\\n\")\n",
    "   df.show()\n",
    "   return f1, precision, recall, acc \n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Confusion Matrix\n",
      "\n",
      "+----+------+-----+\n",
      "|  _1|    _2|   _3|\n",
      "+----+------+-----+\n",
      "| 115|    82|   16|\n",
      "|6440|573829|16115|\n",
      "|   3|    14|   73|\n",
      "+----+------+-----+\n",
      "\n",
      "Printing stats for Logistic classification model\n",
      "\n",
      "Accuracy: 0.9620068813297424\n",
      "F1: 0.9806452825706715\n",
      "Precision: 0.9998327307575031\n",
      "Recall: 0.9621804072543865\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression model\n",
    "ev = LogisticRegression(labelCol='label', featuresCol='features',maxIter=10)\n",
    "lr_f1, lr_precision, lr_recall, lr_acc = model(ev)\n",
    "print(\"Printing stats for Logistic classification model\\n\")\n",
    "print(\"Accuracy: \" + str(lr_acc))\n",
    "print(\"F1: \" + str(lr_f1))\n",
    "print(\"Precision: \" + str(lr_precision))\n",
    "print(\"Recall: \" + str(lr_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Confusion Matrix\n",
      "\n",
      "+----+------+-----+\n",
      "|  _1|    _2|   _3|\n",
      "+----+------+-----+\n",
      "| 110|     4|   15|\n",
      "|6446|573920|16182|\n",
      "|   2|     1|    7|\n",
      "+----+------+-----+\n",
      "\n",
      "Printing stats for Decision Tree classifier model\n",
      "\n",
      "Accuracy: 0.9620403997405675\n",
      "F1: 0.9806633728415778\n",
      "Precision: 0.9999912880602866\n",
      "Recall: 0.962068433722014\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree classifier model\n",
    "ev = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "dt_f1, dt_precision, dt_recall, dt_acc = model(ev)\n",
    "print(\"Printing stats for Decision Tree classifier model\\n\")\n",
    "print(\"Accuracy: \" + str(dt_acc))\n",
    "print(\"F1: \" + str(dt_f1))\n",
    "print(\"Precision: \" + str(dt_precision))\n",
    "print(\"Recall: \" + str(dt_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Confusion Matrix\n",
      "\n",
      "+------+\n",
      "|    _1|\n",
      "+------+\n",
      "|573925|\n",
      "+------+\n",
      "\n",
      "Printing stats for Random Forest classifier model\n",
      "\n",
      "Accuracy: 0.9618526966399469\n",
      "F1: 0.9805554701301541\n",
      "Precision: 1.0\n",
      "Recall: 0.9618526966399469\n"
     ]
    }
   ],
   "source": [
    "# Random Classifier model\n",
    "ev = RandomForestClassifier(labelCol=\"label\",featuresCol=\"features\", numTrees=10)\n",
    "rf_f1, rf_precision, rf_recall, rf_acc = model(ev)\n",
    "print(\"Printing stats for Random Forest classifier model\\n\")\n",
    "print(\"Accuracy: \" + str(rf_acc))\n",
    "print(\"F1: \" + str(rf_f1))\n",
    "print(\"Precision: \" + str(rf_precision))\n",
    "print(\"Recall: \" + str(rf_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable Function with preprocessing, model build and evaluation steps for cross validation\n",
    "def cvmodel(ev,paramGrid):\n",
    "   #Feature Preprocessing steps\n",
    "   categoricalCols = [field for (field, dataType) in training.dtypes if dataType == \"string\"]\n",
    "   indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "   oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "   stringIndexer = StringIndexer(inputCols=categoricalCols,outputCols=indexOutputCols,handleInvalid=\"skip\")\n",
    "   oheEncoder = OneHotEncoder(inputCols=indexOutputCols,outputCols=oheOutputCols)\n",
    "   numericCols = [field for (field, dataType) in training.dtypes if ((dataType == \"double\") & (field != \"label\"))]\n",
    "   assemblerInputs = oheOutputCols + numericCols\n",
    "   va = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\", handleInvalid=\"skip\") \n",
    "   \n",
    "   # Model Build using pipelines\n",
    "   pipeline = Pipeline(stages=[stringIndexer,oheEncoder,va, ev])\n",
    "   crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "   model = crossval.fit(training)\n",
    "    \n",
    "   #Model Evaluation using MulticlassClassificationEvaluator and printing stats for the model\n",
    "   preds = model.transform(test)\n",
    "   mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "   acc = mcEvaluator.evaluate(preds)\n",
    "   predictionAndLabels = model.transform(test).select('label', 'prediction')\n",
    "   metrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))\n",
    "   confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "   labels = [int(l) for l in metrics.call('labels')]\n",
    "   f1 = metrics.fMeasure(1.0)\n",
    "   precision = metrics.precision(1.0)\n",
    "   recall = metrics.recall(1.0)\n",
    "   data = metrics.confusionMatrix().toArray()\n",
    "   rdd1 = spark.sparkContext.parallelize(data)\n",
    "   rdd2 = rdd1.map(lambda x: [int(i) for i in x])\n",
    "   df = rdd2.toDF()\n",
    "   print(\"Printing Confusion Matrix\\n\")\n",
    "   df.show()\n",
    "   return f1, precision, recall, acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Confusion Matrix\n",
      "\n",
      "+----+------+-----+\n",
      "|  _1|    _2|   _3|\n",
      "+----+------+-----+\n",
      "|  85|    59|   11|\n",
      "|6472|573852|16120|\n",
      "|   1|    14|   73|\n",
      "+----+------+-----+\n",
      "\n",
      "Printing stats for CV Logistic classification model\n",
      "\n",
      "Accuracy: 0.9619951498859536\n",
      "F1: 0.9806343127680245\n",
      "Precision: 0.9998728056801847\n",
      "Recall: 0.9621221774382842\n"
     ]
    }
   ],
   "source": [
    "# Cross validation for the Logistic Classifier model\n",
    "ev = LogisticRegression(labelCol='label', featuresCol='features',maxIter=10) #Evaluator for the model\n",
    "\n",
    "# Create ParamGrid for cross validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(ev.regParam, [0.1, 1.0, 2.0]) # Regularization parameter\n",
    "             .addGrid(ev.elasticNetParam, [0.0, 0.5, 1.0]) #Elastic Net Paramater (Ridge = 0)\n",
    "             .addGrid(ev.maxIter, [1, 5, 10]) #Number of Iterations\n",
    "             .build())\n",
    "cv_lr_f1, cv_lr_precision, cv_lr_recall, cv_lr_acc = cvmodel(ev,paramGrid)\n",
    "print(\"Printing stats for CV Logistic classification model\\n\")\n",
    "print(\"Accuracy: \" + str(cv_lr_acc))\n",
    "print(\"F1: \" + str(cv_lr_f1))\n",
    "print(\"Precision: \" + str(cv_lr_precision))\n",
    "print(\"Recall: \" + str(cv_lr_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Confusion Matrix\n",
      "\n",
      "+----+------+-----+\n",
      "|  _1|    _2|   _3|\n",
      "+----+------+-----+\n",
      "| 162|    16|   26|\n",
      "|6388|573900|16129|\n",
      "|   8|     9|   49|\n",
      "+----+------+-----+\n",
      "\n",
      "Printing stats for CV Decision Tree classifier model\n",
      "\n",
      "Accuracy: 0.9621644178606203\n",
      "F1: 0.9807389634824694\n",
      "Precision: 0.9999564403014332\n",
      "Recall: 0.9622462136391149\n"
     ]
    }
   ],
   "source": [
    "# Cross validation for the Decision Tree Classifier model\n",
    "ev = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")  #Evaluator for the model\n",
    "\n",
    "# Create ParamGrid for cross validation\n",
    "paramGrid = (ParamGridBuilder()\\\n",
    "            .addGrid(ev.maxDepth, [2, 5, 10]) # number of splits for each tree\n",
    "            .addGrid(ev.maxBins, [5, 10 , 20]) # Number of bins\n",
    "            .build())\n",
    "cv_dt_f1, cv_dt_precision, cv_dt_recall, cv_dt_acc = cvmodel(ev,paramGrid)\n",
    "print(\"Printing stats for CV Decision Tree classifier model\\n\")\n",
    "print(\"Accuracy: \" + str(cv_dt_acc))\n",
    "print(\"F1: \" + str(cv_dt_f1))\n",
    "print(\"Precision: \" + str(cv_dt_precision))\n",
    "print(\"Recall: \" + str(cv_dt_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Confusion Matrix\n",
      "\n",
      "+------+\n",
      "|    _1|\n",
      "+------+\n",
      "|573925|\n",
      "+------+\n",
      "\n",
      "Printing stats for CV Random Forest classifier model\n",
      "\n",
      "Accuracy: 0.9618526966399469\n",
      "F1: 0.9805554701301541\n",
      "Precision: 1.0\n",
      "Recall: 0.9618526966399469\n"
     ]
    }
   ],
   "source": [
    "# Cross validation for the RF model\n",
    "ev = RandomForestClassifier()\n",
    "paramGrid = (ParamGridBuilder()\\\n",
    "            .addGrid(ev.maxDepth, [2, 5, 10]) # number of splits for each tree\n",
    "            .addGrid(ev.maxBins, [5, 10 , 20]) # Number of bins\n",
    "            .addGrid(ev.numTrees,[5, 20, 50]) # Number of decision trees\n",
    "            .build())\n",
    "\n",
    "cv_rf_f1, cv_rf_precision, cv_rf_recall, cv_rf_acc = cvmodel(ev,paramGrid)\n",
    "print(\"Printing stats for CV Random Forest classifier model\\n\")\n",
    "print(\"Accuracy: \" + str(cv_rf_acc))\n",
    "print(\"F1: \" + str(cv_rf_f1))\n",
    "print(\"Precision: \" + str(cv_rf_precision))\n",
    "print(\"Recall: \" + str(cv_rf_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------------------+------------------+------------------+------------------+\n",
      "|Model                      |Accuracy          |F1 Score          |Precision         |Recall            |\n",
      "+---------------------------+------------------+------------------+------------------+------------------+\n",
      "|Logistic Regression        |0.9620068813297424|0.9806452825706715|0.9998327307575031|0.9621804072543865|\n",
      "|CV Logistic Regression     |0.9619951498859536|0.9806343127680245|0.9998728056801847|0.9621221774382842|\n",
      "|Decision Tree Classfier    |0.9620403997405675|0.9806633728415778|0.9999912880602866|0.962068433722014 |\n",
      "|CV Decision Tree Classfier |0.9621644178606203|0.9807389634824694|0.9999564403014332|0.9622462136391149|\n",
      "|Random Forest Classifier   |0.9618526966399469|0.9805554701301541|1.0               |0.9618526966399469|\n",
      "|CV Random Forest Classifier|0.9618526966399469|0.9805554701301541|1.0               |0.9618526966399469|\n",
      "+---------------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe from the results of different models built above\n",
    "columns = [\"Model\",\"Accuracy\",\"F1 Score\", \"Precision\", \"Recall\"]\n",
    "data = [(\"Logistic Regression\", lr_acc, lr_f1, lr_precision, lr_recall), \n",
    "        ('CV Logistic Regression', cv_lr_acc, cv_lr_f1, cv_lr_precision, cv_lr_recall), \n",
    "        ('Decision Tree Classfier', dt_acc, dt_f1, dt_precision, dt_recall), \n",
    "        ('CV Decision Tree Classfier', cv_dt_acc, cv_dt_f1, cv_dt_precision, cv_dt_recall),\n",
    "        ('Random Forest Classifier', rf_acc, rf_f1, rf_precision, rf_recall), \n",
    "        ('CV Random Forest Classifier', cv_rf_acc, cv_rf_f1, cv_rf_precision, cv_rf_recall)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "acc_df = spark.createDataFrame(rdd).toDF(*columns)\n",
    "acc_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
